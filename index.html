<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Given a video and a text prompt, Dreamix edits the video while maintaining fidelity to color, posture, object size and camera pose, resulting in a temporally consistent video.">
  <meta property="og:title" content="Explainable Driver Activity Recognition Using Video Transformer in Highly Automated Vehicle"/>
  <meta property="og:description" content="Given a video and a text prompt, Dreamix edits the video while maintaining fidelity to color, posture, object size and camera pose, resulting in a temporally consistent video."/>
  <meta property="og:url" content="https://dreamix-video-editing.github.io"/>
  <meta property="og:image" content="static/image/og_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Explainable Driver Activity Recognition Using Video Transformer in Highly Automated Vehicle">
  <meta name="twitter:description" content="Given a video and a text prompt, Dreamix edits the video while maintaining fidelity to color, posture, object size and camera pose, resulting in a temporally consistent video.">
  <meta name="twitter:image" content="static/images/twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Video diffusion models, video editing, dreamix, dreambooth, imagen-video">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Explainable Driver Activity Recognition Using Video Transformer in Highly Automated Vehicle</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Explainable Driver Activity Recognition Using Video Transformer in Highly Automated Vehicle</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
             <span class="author-block">
                <a href="https://www.linkedin.com/in/akashsonth" target="_blank">Akash Sonth</a><sup>1,2</sup>
              </span>
              <span class="author-block">
                <a href="https://www.vtti.vt.edu/staffdir/bio.php?&pn=119388" target="_blank">Abhijit Sarkar</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/hirva-bhagat" target="_blank">Hirva Bhagat</a><sup>2,3</sup>
              </span>              
              <span class="author-block">
                <a href="https://ece.vt.edu/people/profile/abbott.html" target="_blank">Lynn Abbott</a><sup>2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Virginia Tech Transportation Institute,</span>
              <span class="author-block"><sup>2</sup>Bradley Department of Electrical and Computer Engineering, Virginia Tech</span>
              <span class="author-block"><sup>3</sup>Department of Computer Science, Virginia Tech</span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution, <sup>â€ </sup>Indicates Equal Advising</small></span> -->
              <span class="eql-cntrb"><small></small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10186584/authors#authors" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2302.01329" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
              </div>
            </div>
      </div>
    </div>
  </div>
</div>
</section>








<!-- Subject Driven Video Generation carousel -->
<section class="hero teaser">
      <div class="hero-body ">
    <div class="container is-max-desktop">
  
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-gif1">
          <center>
          <img src="static/images/cellphone_talking.gif" alt="Spatial variation in attention" width="500" height="500"/>
          </center>
        </div>

        <div class="item item-gif2">
          <center>
          <img src="static/images/cellphone_texting.gif" alt="Spatial variation in attention" width="500" height="500"/>
          </center>
        </div>

        <div class="item item-gif3">
          <center>
          <img src="static/images/center_stack_adjusting.gif" alt="Spatial variation in attention" width="500" height="500"/>
          </center>
        </div>

        <div class="item item-gif4">
          <center>
          <img src="static/images/drinking.gif" alt="Spatial variation in attention" width="500" height="500"/>
          </center>
        </div>

        <div class="item item-gif5">
          <center>
          <img src="static/images/eating.gif" alt="Spatial variation in attention" width="500" height="500"/>
          </center>
        </div>        

        <div class="item item-gif6">
          <center>
          <img src="static/images/sun_visor_adjusting.gif" alt="Spatial variation in attention" width="500" height="500"/>
          </center>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End Subject Driven Video Generation carousel -->







<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Distracted driving is one of the leading causes of road accidents. With the recent introduction of advanced
            driver assistance systems and L2 vehicles, the role of driver attention has gained renewed interest. It is imperative for
            vehicle manufacturers to develop robust systems that can identify distractions and aid in preventing such accidents in highly
            automated vehicles. This paper mainly focuses on studying secondary behaviors, and their relative complexity to develop
            a guide for auto manufacturers. In recent years, a few driver secondary action datasets and deep learning algorithms have
            been created to address this problem. Despite their success in many domains, Convolutional Neural Network based deep
            learning methods struggle to fully consider the overall context of an image, and focus on specific image features. We present
            the use of Video Transformers on two challenging datasets, one of them being a grayscale low-quality dataset. We also
            demonstrate how the novel concept of a Visual Dictionary can be used to understand the structural components of any secondary
            behavior. Finally, we validate different components of the visual dictionary by studying the attention modules of the transformer
            based model and incorporating explainability in the computer vision model. An activity is decomposed into multiple small
            actions and attributes and the corresponding attention patches are highlighted in the input frame.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method Overview: Video Swin-B Transformer</h2>
          <center>
          <img src="static/images/swin.svg" alt="Video Swin-B Transformer" width="1000" height="600"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
               Overall architecture of the Video Swin-B Transformer. 
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Explainable Action Recognition</h2>

          <center>
          <img src="static/images/spatial.svg" alt="Spatial variation in attention" width="1000" height="600"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Variation in attention across various channels from the same layer of the network for 'Cellphone talking'
            </p>
          </div>

          <div style="margin: 2rem 0;"></div>

          <center>
          <img src="static/images/temporal.svg" alt="Temporal variation in attention" width="1000" height="600"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Left to right: Temporal variation in attention as the driver finishes eating and reaches for the bag of chips 
            </p>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@INPROCEEDINGS{10186584,
  author={Sonth, Akash and Sarkar, Abhijit and Bhagat, Hirva and Abbott, Lynn},
  booktitle={2023 IEEE Intelligent Vehicles Symposium (IV)}, 
  title={Explainable Driver Activity Recognition Using Video Transformer in Highly Automated Vehicle}, 
  year={2023},
  pages={1-8},
  keywords={Deep learning;Visualization;Computer vision;Dictionaries;Codes;Computational modeling;Transformers;transformers;action recognition;explainable AI;visual dictionary},
  doi={10.1109/IV55152.2023.10186584}}
</code></pre>
  </div>
</section>
<!-- End BibTex citation -->

<!--Acknowledgements -->
<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
      The authors would like to thank Calvin Winkowski, Stuart
      Walker, and other members of Virginia Tech Transportation
      Institute for their help in accessing the datasets. 
  </div>
</section>
<!--End Acknowledgements -->
  
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- Default Statcounter code for Dreamix
https://dreamix-video-editing.github.io -->
<script type="text/javascript">
var sc_project=12843789; 
var sc_invisible=1; 
var sc_security="e9c3bf5f"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12843789/0/e9c3bf5f/1/"
alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>

<!-- End of Statcounter Code -->

</body>
</html>
